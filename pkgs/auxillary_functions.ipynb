{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05845873",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_modeller' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m textwrap\u001b[38;5;241m.\u001b[39mfill(x, replace_whitespace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, fix_sentence_endings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Calculate the coherence and perplexity scores\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoherence_perp_logl\u001b[39m(sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vectorizer\u001b[38;5;241m=\u001b[39m\u001b[43mlda_modeller\u001b[49m\u001b[38;5;241m.\u001b[39mvectorizer):\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m sentences \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lda_modeller' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot top words per topic\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words = 7):\n",
    "  fig, axes = plt.subplots(1, 5, figsize = (30, 10), sharex = True)\n",
    "  axes = axes.flatten()\n",
    "  for topic_idx, topic in enumerate(model.components_):\n",
    "    top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    weights = topic[top_features_ind]\n",
    "    \n",
    "    ax = axes[topic_idx]\n",
    "    ax.barh(top_features, weights, height = 0.7)\n",
    "    ax.set_title(f'Topic {topic_idx + 1}', fontdict = {'fontsize': 30})\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = 20)\n",
    "    for i in 'top right left'.split():\n",
    "      ax.spines[i].set_visible(False)\n",
    "    fig.suptitle(\"LDA\", fontsize = 40)\n",
    "  \n",
    "  plt.subplots_adjust(top = 0.90, bottom = 0.05, wspace = 0.90, hspace = 0.3)\n",
    "  plt.show()\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "# Estimate model accuracy:\n",
    "def est_accuracy(topic_vocab=None):\n",
    "  new_sentences = lda_modeller.vectorizer.transform(df['text'])\n",
    "  topic_distributions = lda.transform(new_sentences)\n",
    "  most_probable_topics = topic_distributions.argmax(axis=1)\n",
    "  preds = [topic_vocab[num_topic] for num_topic in most_probable_topics]\n",
    "  df['preds'] = preds\n",
    "  acc = sum(df['labels'] == df['preds']) / len(df)\n",
    "  print(f'Supposed model accuracy: {acc}')\n",
    "  \n",
    "  \n",
    "   \n",
    "# Extract latent variables and sample a topic\n",
    "def sample_topic(rand_seed=0, topic_vocab=None, num_topics=5, print_article=False):\n",
    "  Z = lda.transform(lda_modeller.vectorized_sentences)\n",
    "  np.random.seed(rand_seed)\n",
    "  i = np.random.choice(len(df))\n",
    "  z = Z[i]\n",
    "  topics = np.arange(num_topics) + 1\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.barh(topics, z)\n",
    "  ax.set_yticks(topics)\n",
    "  ax.set_title('True label: %s' % df.iloc[i]['labels'])\n",
    "  \n",
    "  if print_article:\n",
    "    print(wrap(df.iloc[i]['text']))\n",
    "\n",
    "  \n",
    "# Text wrapper\n",
    "def wrap(x):\n",
    "  return textwrap.fill(x, replace_whitespace = False, fix_sentence_endings = True)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the coherence and perplexity scores\n",
    "def coherence_perp_logl(sentences=None, vectorizer=None):\n",
    "  if sentences == None:\n",
    "    sentences = df['text']\n",
    "    \n",
    "  if vectorizer == None:\n",
    "    vectorizer = lda_modeller.vectorizer\n",
    "    \n",
    "  vocab = vectorizer.get_feature_names_out()\n",
    "  words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "  id2word = corpora.Dictionary(words)\n",
    "  \n",
    "  topics = []\n",
    "  for i in range(lda.n_components):\n",
    "      topic_words = [vocab[j] for j in lda.components_[i].argsort()[:-10 - 1:-1]]\n",
    "      topics.append(topic_words)\n",
    "\n",
    "  coherence_model_lda = CoherenceModel(topics=topics, texts=words, dictionary=id2word, coherence='c_v')\n",
    "  coherence_lda = coherence_model_lda.get_coherence()\n",
    "  perplexity = np.log(lda.perplexity(lda_modeller.vectorized_sentences))\n",
    "  log_likelihood = lda.score(lda_modeller.vectorized_sentences)\n",
    "  print(f'The coherence score is: {round(coherence_lda, 3)}')\n",
    "  print(f'The log perplexity score is: {round(perplexity, 3)}')\n",
    "  print(f'The log likelihood is: {round(log_likelihood, 3)}')\n",
    "  \n",
    "  return vocab, words, id2word\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "def metrics_model_selection(dictionary, words, limit=10, start=2, step=1, topic_word_prior=0.6):\n",
    "  \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    perplexity_list = []\n",
    "    log_likelihood_scores = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LatentDirichletAllocation(n_components = num_topics, max_iter = 100, topic_word_prior = topic_word_prior\n",
    "#                                          learning_method='online'\n",
    "                                         )\n",
    "        model.fit(lda_modeller.vectorized_sentences) \n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for i in range(model.n_components):\n",
    "            topic_words = [vocab[j] for j in model.components_[i].argsort()[:-10 - 1:-1]]\n",
    "            topics.append(topic_words)\n",
    "        \n",
    "        coherencemodel = CoherenceModel(topics=topics, texts=words, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "        perplexity = np.log(model.perplexity(lda_modeller.vectorized_sentences))\n",
    "        perplexity_list.append(perplexity)\n",
    "        \n",
    "        log_likelihood = model.score(lda_modeller.vectorized_sentences)\n",
    "        log_likelihood_scores.append(log_likelihood)\n",
    "        \n",
    "        \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "    x = range(start, limit, step)\n",
    "\n",
    "    axs[0].plot(x, coherence_values)\n",
    "    axs[0].set_title('Coherence')\n",
    "    axs[0].set_xlabel('Num Topics')\n",
    "    axs[0].set_ylabel('Coherence Score')\n",
    "\n",
    "    axs[1].plot(x, perplexity_list)\n",
    "    axs[1].set_title('Perplexity')\n",
    "    axs[1].set_xlabel('Num Topics')\n",
    "    axs[1].set_ylabel('Perplexity Score')\n",
    "\n",
    "    axs[2].plot(x, log_likelihood_scores)\n",
    "    axs[2].set_title('Log likelihood')\n",
    "    axs[2].set_xlabel('Num Topics')\n",
    "    axs[2].set_ylabel('Log-likelihood Score')\n",
    "\n",
    "    return model_list, coherence_values, perplexity_list, log_likelihood_scores\n",
    "  \n",
    "  \n",
    "\n",
    "# Print top words per topic  \n",
    "def print_n_words_per_topic(num_words=15):\n",
    "  vocab = np.array(lda_modeller.vectorizer.get_feature_names_out())\n",
    "  top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "  topic_words = ([top_words(t) for t in H1])\n",
    "  topics = [' '.join(t) for t in topic_words]\n",
    "  for topic in topics:\n",
    "    print(topic)\n",
    "    print('\\n')\n",
    "    \n",
    "      \n",
    "    \n",
    "# Topic per document matrix\n",
    "def topic_per_doc_matrix():\n",
    "  colnames = ['Topic' + str(i) for i in range(lda.n_components)]\n",
    "  docnames = ['Doc' + str(i) for i in range(len(df['text']))]\n",
    "  df_doc_topic = pd.DataFrame(np.round(W1, 2), columns = colnames, index = docnames)\n",
    "  df_doc_topic['dominant_topic'] = np.argmax(df_doc_topic.values, axis = 1)\n",
    "  return df_doc_topic\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf9013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca6729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
