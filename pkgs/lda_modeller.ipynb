{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a923503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAmodellerSKL:\n",
    "  def __init__(self, df, vectorizer, stops=stops, min_df=2, max_df=0.95, num_topics=5, lda_random_state=12345, max_iter=100, topic_word_prior=0.6):\n",
    "    self.df = df\n",
    "    \n",
    "    if vectorizer == 'count':\n",
    "      self.vectorizer = CountVectorizer(stop_words = list(stops),\n",
    "                                        min_df = min_df,\n",
    "                                        max_df = max_df)\n",
    "      \n",
    "    elif vectorizer == 'tfidf':\n",
    "      self.vectorizer = TfidfVectorizer(stop_words = list(stops), \n",
    "                                max_df = max_df, \n",
    "                                min_df = min_df, \n",
    "                                use_idf = True,\n",
    "                                norm = None)\n",
    "      \n",
    "    else:\n",
    "      raise ValueError('The vectorizer value can be either \"count\" or \"tfidf\"')\n",
    "    \n",
    "    \n",
    "    \n",
    "    self.lda = LatentDirichletAllocation(n_components = num_topics,\n",
    "                                         random_state = lda_random_state,\n",
    "                                         max_iter = max_iter,\n",
    "                                         topic_word_prior = topic_word_prior)\n",
    "    \n",
    "    self.vectorized_sentences = None\n",
    "    self.new_vectorized_sentences = None\n",
    "  \n",
    "  def vectorize(self):\n",
    "    self.new_vectorized_sentences = self.vectorizer.fit_transform(self.df['text'])\n",
    "    return self.new_vectorized_sentences\n",
    "    \n",
    "  \n",
    "  def vectorize_fit(self):\n",
    "    self.vectorized_sentences = self.vectorizer.fit_transform(self.df['text'])\n",
    "    self.lda.fit(self.vectorized_sentences)\n",
    "    return self.lda\n",
    "  \n",
    "  def transform(self, sentences=None):\n",
    "    if sentences is None:\n",
    "      vectors = self.lda.transform(self.vectorized_sentences)\n",
    "    else:\n",
    "      vectors = self.lda.transform(sentences)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a708111c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLDAmodellerGensim\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, vectorizer, stops\u001b[38;5;241m=\u001b[39mstops, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lda_random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m, ):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mLDAmodellerGensim\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLDAmodellerGensim\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, vectorizer, stops\u001b[38;5;241m=\u001b[39m\u001b[43mstops\u001b[49m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lda_random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m, ):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vectorizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stops' is not defined"
     ]
    }
   ],
   "source": [
    "class LDAmodellerGensim:\n",
    "  def __init__(self, df, vectorizer, stops=stops, min_df=2, max_df=0.95, num_topics=5, lda_random_state=12345, \n",
    "               passes=200, iterations=200, per_words_topic=False, alpha=8, eta=0.9, gamma_threshold=8):\n",
    "    \n",
    "    self.df = df\n",
    "    self.stops = stops\n",
    "    self.min_df = min_df\n",
    "    self.max_df = max_df\n",
    "    self.num_topics = num_topics\n",
    "    self.lda_random_state = lda_random_state\n",
    "    self.passes = passes\n",
    "    self.iterations = iterations\n",
    "    self.per_words_topic = per_words_topic\n",
    "    self.alpha = alpha\n",
    "    self.eta = eta\n",
    "    self.gamma_threshold = gamma_threshold\n",
    "    \n",
    "    if vectorizer == 'count':\n",
    "      self.vectorizer = CountVectorizer(stop_words = list(stops),\n",
    "                                        min_df = min_df,\n",
    "                                        max_df = max_df)\n",
    "      \n",
    "    elif vectorizer == 'tfidf':\n",
    "      self.vectorizer = TfidfVectorizer(stop_words = list(stops), \n",
    "                                max_df = max_df, \n",
    "                                min_df = min_df, \n",
    "                                use_idf = True,\n",
    "                                norm = None)\n",
    "      \n",
    "    else:\n",
    "      raise ValueError('The vectorizer value can be either \"count\" or \"tfidf\"')\n",
    "    \n",
    "    self.vectorizer.fit(df['text'])\n",
    "    self.vocab = self.vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Create document-term matrix\n",
    "  def create_document_term_matrix(self, column_name='text', stops=stops, min_df=2, max_df=0.95):\n",
    "    data = self.vectorizer.fit_transform(self.df[column_name])\n",
    "    df_dtm = pd.DataFrame(data.toarray(), columns = self.vectorizer.get_feature_names_out())\n",
    "    df_dtm.index = self.df.index\n",
    "    return df_dtm\n",
    "    \n",
    "    \n",
    "  def make_bigrams(self, texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "  def make_trigrams(self, texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "  \n",
    "    \n",
    "  def create_ngrams(self):\n",
    "    words = words = [nltk.word_tokenize(sentence) for sentence in self.df['text']]\n",
    "    bigram = gensim.models.Phrases(words, min_count = 5, threshold = 100)\n",
    "    trigram = gensim.models.Phrases(bigram[words], threshold = 100)\n",
    "  \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "   \n",
    "    word_bigrams = self.make_bigrams(words, bigram_mod = bigram_mod)\n",
    "    word_trigrams = self.make_trigrams(words, bigram_mod = bigram_mod, trigram_mod = trigram_mod)\n",
    "      \n",
    "    self.words = words\n",
    "    self.word_bigrams = word_bigrams\n",
    "    self.word_trigrams = word_trigrams\n",
    "      \n",
    "    \n",
    "  def id2word_corpus(self):\n",
    "    self.id2word = corpora.Dictionary(self.words)\n",
    "    # OPTIONAL STEP!.\n",
    "    # Filter out tokens that appear in less than 15 documents, more than 0.5 documents (fraction of total corpus size, not absolute number) and keep only the first 100000 most frequent tokens.\n",
    "    # id2word.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    self.corpus = [self.id2word.doc2bow(word) for word in self.words]\n",
    "\n",
    "     \n",
    "  def train_lda(self):\n",
    "    self.create_document_term_matrix()\n",
    "    self.create_ngrams()\n",
    "    self.id2word_corpus()\n",
    "    \n",
    "    self.lda_model = gensim.models.LdaMulticore(corpus = self.corpus, \n",
    "                                           id2word = self.id2word, \n",
    "                                           num_topics = self.num_topics, \n",
    "                                           passes = self.passes,\n",
    "                                           iterations = self.iterations,\n",
    "                                           per_word_topics = self.per_words_topic,\n",
    "                                           random_state = self.lda_random_state,\n",
    "                                           alpha = self.alpha,\n",
    "                                           eta = self.eta,\n",
    "                                           gamma_threshold = self.gamma_threshold\n",
    "                                          )\n",
    "      \n",
    "    pprint(self.lda_model.print_topics())\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAmodeller:\n",
    "  def __init__(self, df, vectorizer, lib, stops=stops, min_df=2, max_df=0.95, \n",
    "               num_topics=5, lda_random_state=12345, max_iter=100, topic_word_prior=0.6,\n",
    "               passes=200, iterations=200, per_words_topic=False, alpha=8, eta=0.9, \n",
    "               gamma_threshold=8, verbose=True):\n",
    "  \n",
    "    self.df = df\n",
    "    self.lib = lib\n",
    "    self.stops = stops\n",
    "    self.min_df = min_df\n",
    "    self.max_df = max_df\n",
    "    self.num_topics = num_topics\n",
    "    self.lda_random_state = lda_random_state\n",
    "    self.max_iter = max_iter\n",
    "    self.topic_word_prior = topic_word_prior\n",
    "    self.passes = passes\n",
    "    self.iterations = iterations\n",
    "    self.per_words_topic = per_words_topic\n",
    "    self.alpha = alpha\n",
    "    self.eta = eta\n",
    "    self.gamma_threshold = gamma_threshold\n",
    "    self.verbose = verbose\n",
    "  \n",
    "    if vectorizer == 'count':\n",
    "      self.vectorizer = CountVectorizer(stop_words = list(stops),\n",
    "                                        min_df = min_df,\n",
    "                                        max_df = max_df)\n",
    "      \n",
    "    elif vectorizer == 'tfidf':\n",
    "      self.vectorizer = TfidfVectorizer(stop_words = list(stops), \n",
    "                                max_df = max_df, \n",
    "                                min_df = min_df, \n",
    "                                use_idf = True,\n",
    "                                norm = None)\n",
    "      \n",
    "    else:\n",
    "      raise ValueError('The vectorizer value can be either \"count\" or \"tfidf\"')\n",
    "    \n",
    "    \n",
    "    self.vectorizer.fit(df['text'])\n",
    "    self.vocab = self.vectorizer.get_feature_names_out()\n",
    "    self.vectorized_sentences = self.vectorizer.fit_transform(self.df['text'])\n",
    "      \n",
    "      \n",
    "  def create_document_term_matrix(self, column_name='text', stops=stops, min_df=2, max_df=0.95):\n",
    "    data = self.vectorizer.fit_transform(self.df[column_name])\n",
    "    df_dtm = pd.DataFrame(data.toarray(), columns = self.vectorizer.get_feature_names_out())\n",
    "    df_dtm.index = self.df.index\n",
    "    return df_dtm\n",
    "    \n",
    "    \n",
    "  def make_bigrams(self, texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "  \n",
    "  def make_trigrams(self, texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "  \n",
    "  \n",
    "  def create_ngrams(self):\n",
    "    words = words = [nltk.word_tokenize(sentence) for sentence in self.df['text']]\n",
    "    bigram = gensim.models.Phrases(words, min_count = 5, threshold = 100)\n",
    "    trigram = gensim.models.Phrases(bigram[words], threshold = 100)\n",
    "\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    word_bigrams = self.make_bigrams(words, bigram_mod = bigram_mod)\n",
    "    word_trigrams = self.make_trigrams(words, bigram_mod = bigram_mod, trigram_mod = trigram_mod)\n",
    "\n",
    "    self.words = words\n",
    "    self.word_bigrams = word_bigrams\n",
    "    self.word_trigrams = word_trigrams\n",
    "    \n",
    "    \n",
    "  def id2word_corpus(self):\n",
    "    self.id2word = corpora.Dictionary(self.words)\n",
    "    # OPTIONAL STEP!.\n",
    "    # Filter out tokens that appear in less than 15 documents, more than 0.5 documents (fraction of total corpus size, not absolute number) and keep only the first 100000 most frequent tokens.\n",
    "    # id2word.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    self.corpus = [self.id2word.doc2bow(word) for word in self.words]\n",
    "    \n",
    "    \n",
    "    \n",
    "  def train_lda(self):\n",
    "    if self.lib == 'skl':\n",
    "      self.lda_model = LatentDirichletAllocation(n_components = self.num_topics,\n",
    "                                           random_state = self.lda_random_state,\n",
    "                                           max_iter = self.max_iter,\n",
    "                                           topic_word_prior = self.topic_word_prior)\n",
    "        \n",
    "      self.lda_model.fit(self.vectorized_sentences)\n",
    "      \n",
    "      if self.verbose:\n",
    "        plot_top_words(self.lda_model, self.vocab)\n",
    "      \n",
    "    elif self.lib == 'gensim':\n",
    "      self.create_document_term_matrix()\n",
    "      self.create_ngrams()\n",
    "      self.id2word_corpus()\n",
    "      \n",
    "      self.lda_model = gensim.models.LdaMulticore(corpus = self.corpus, \n",
    "                                           id2word = self.id2word, \n",
    "                                           num_topics = self.num_topics, \n",
    "                                           passes = self.passes,\n",
    "                                           iterations = self.iterations,\n",
    "                                           per_word_topics = self.per_words_topic,\n",
    "                                           random_state = self.lda_random_state,\n",
    "                                           alpha = self.alpha,\n",
    "                                           eta = self.eta,\n",
    "                                           gamma_threshold = self.gamma_threshold\n",
    "                                          )\n",
    "      \n",
    "      if self.verbose:\n",
    "        pprint(self.lda_model.print_topics())\n",
    "      \n",
    "    else:\n",
    "      raise ValueError('Lib parameter can be either \"skl\" or \"gensim\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
